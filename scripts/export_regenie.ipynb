{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script used to reformat and export Hail covariate and phenotype tables for use with Regenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UKBbucket_root = 'gs://rbif120data'\n",
    "UKBbucket = UKBbucket_root + '/ukb/'\n",
    "UKBbucket_pheno = UKBbucket + 'pheno/'\n",
    "import hail as hl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to fix spark ui problem\n",
    "import os\n",
    "os.environ.pop(\"spark.ui.proxyBase\", None)\n",
    "#sys.props.update(\"spark.ui.proxyBase\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Unset any existing PYSPARK_SUBMIT_ARGS\n",
    "os.environ.pop(\"PYSPARK_SUBMIT_ARGS\", None)\n",
    "\n",
    "hail_jar_path = \"/opt/conda/lib/python3.10/site-packages/hail/backend/hail-all-spark.jar\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--jars {hail_jar_path} --driver-class-path {hail_jar_path} --conf spark.executor.extraClassPath=./hail-all-spark.jar pyspark-shell'\n",
    "\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\n",
    "conf = SparkConf().setAppName(\"Hail\") \\\n",
    "    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .set(\"spark.kryo.registrator\", \"is.hail.kryo.HailKryoRegistrator\") \\\n",
    "    .set(\"spark.driver.memory\", \"32g\") \\\n",
    "    .set(\"spark.ui.proxyBase\", \"\") \\\n",
    "    .set(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .set(\"spark.kryoserializer.buffer.max\", \"2047m\") \\\n",
    "    .set(\"spark.jars\", hail_jar_path) \\\n",
    "    .set(\"spark.driver.extraClassPath\", hail_jar_path) \\\n",
    "    .set(\"spark.executor.extraClassPath\", \"./hail-all-spark.jar\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "hail_context = hl.init(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Hail covariates table to text file for Regenie input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "td_pro=hl.read_table(UKBbucket_pheno + \"npx_processing\")\n",
    "td_pro.count()\n",
    "\n",
    "tablePhe=hl.read_table(UKBbucket_pheno + \"AllPhenosCat2\")\n",
    "\n",
    "tablePhe = tablePhe.annotate(\n",
    "    age_squared = tablePhe.age ** 2,\n",
    "    age_sex = tablePhe.age * (tablePhe.sex +1),\n",
    "    age_squared_sex = (tablePhe.age ** 2) * (tablePhe.sex +1)\n",
    ")\n",
    "\n",
    "tablePhe= tablePhe.join(td_pro)\n",
    "\n",
    "covariates=['Batch','days_to_olink','age','sex','age_squared','age_sex','age_squared_sex','PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']\n",
    "donttest=['gSex','gMissing','Caucasian','Aneupleoidy','hetOutlier']\n",
    "\n",
    "# tablePhe.head(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select desired covariates from Pheno table\n",
    "tableCov = tablePhe.select(*covariates)\n",
    "# tableCov.head(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableCov.export(UKBbucket + \"regenie/covariates.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert olink protein data (.csv) to regenie input format (tab-separated .txt w/ FID and IID as first 2 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(UKBbucket + \"npx/npx_rint.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cols[-1:] + cols[-1:] + cols[:-1]\n",
    "#cols[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[cols]\n",
    "#df.columns.tolist()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['FID', 'IID'] + cols[2:]\n",
    "#df.columns.tolist()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(UKBbucket + \"regenie/phenotype_bin.txt\", sep='\\t', index=False, na_rep='NA')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
